{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      },
      "source": [
        "# ‚≠ê Entrenador de Lora de Hollowstrawberry\n",
        "\n",
        "Este colab viene de [esta gu√≠a](https://huggingface.co/hollowstrawberry/stable-diffusion-guide/blob/main/spanish.md#index). Te guiar√° para obtener tus im√°genes y descripciones r√°pidamente, para luego usarlas para entrenar un Lora.\n",
        "\n",
        "Basado en el trabajo de [Kohya_ss y Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb#scrollTo=-Z4w3lfFKLjr). ¬°Gracias!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y144kNjGN-jN"
      },
      "source": [
        "| |GitHub|üá¨üáß English|üá™üá∏ Spanish|\n",
        "|:--|:-:|:-:|:-:|\n",
        "| üìä **Dataset Maker** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) |\n",
        "| ‚≠ê **Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OglZzI_ujZq-"
      },
      "outputs": [],
      "source": [
        "#@title ## üö© Empieza Aqu√≠\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "from google.colab import output as console\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Inicio\n",
        "#@markdown El nombre de tu proyecto tambi√©n es el nombre de la carpeta donde ir√°n tus im√°genes. No se permiten espacios.\n",
        "nombre_proyecto = \"\" #@param {type:\"string\"}\n",
        "project_name = nombre_proyecto\n",
        "#@markdown Decidir el modelo base de entrenamiento. Los modelos por defecto producen los resultados m√°s limpios y consistentes. Puedes cambiarlo por un modelo propio si lo deseas.\n",
        "modelo_de_entrenamiento = \"Anime (animefull-final-pruned-fp16.safetensors)\" #@param [\"Anime (animefull-final-pruned-fp16.safetensors)\", \"Fotorealismo (sd-v1-5-pruned-noema-fp16.safetensors)\"]\n",
        "opcional_modelo_propio = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if opcional_modelo_propio:\n",
        "  model_url = opcional_modelo_propio\n",
        "elif \"Anime\" in modelo_de_entrenamiento:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n",
        "else:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Archivos <p>\n",
        "#@markdown Si usaste mi [preparador de Lora](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb#scrollTo=0BJTp5PVN_q-), est√°s listo. Sino, crea una carpeta en tu Google Drive as√≠: `lora_training/datasets/nombre_proyecto` y ll√©nala con tus im√°genes y textos. Puedes descomprimir un archivo zip en los Extras m√°s abajo. <p>\n",
        "#@markdown La resoluci√≥n de 512 es est√°ndar en Stable Diffusion 1.5. No es necesario recortar o achicar, el proceso es autom√°tico.\n",
        "resolucion = 512 #@param {type:\"number\"}\n",
        "resolution = resolucion\n",
        "#@markdown Esta opci√≥n va a voltear tus im√°genes para as√≠ tener el doble y aprender mejor. <p>\n",
        "#@markdown **Desactiva esto si te importan los elementos asim√©tricos en tu Lora.**\n",
        "flip_aug = True #@param {type:\"boolean\"}\n",
        "#@markdown Si tienes una palabra de activaci√≥n al inicio de tus archivos de texto, aumenta este valor a 1.\n",
        "keep_tokens = 1 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "#@markdown Si usas tags de anime mant√©n esto activo.\n",
        "shuffle_caption = True #@param {type:\"boolean\"}\n",
        "caption_extension = \".txt\"\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Pasos <p>\n",
        "#@markdown Tus im√°genes se repetir√°n durante el entrenamiento. Recomiendo que tu cantidad de im√°genes multiplicada por las repeticiones est√© entre 200 y 400.\n",
        "repeticiones = 10 #@param {type:\"number\"}\n",
        "num_repeats = repeticiones\n",
        "#@markdown Un epoch es una cantidad de pasos equivalente a tu cantidad de im√°genes multiplicada por las repeticiones y dividido por el batch size. <p>\n",
        "#@markdown M√°s epochs le dar√° mas tiempo a tu Lora para aprender y m√°s opciones para que puedas comparar. Recomiendo entre 10 y 30 epochs, lo que deber√≠a resultar en 1000 a 6000 pasos totales. Ver√°s tus pasos al empezar el entrenamiento.\n",
        "epochs = 15 #@param {type:\"number\"}\n",
        "max_train_epochs = epochs\n",
        "guardar_cada_cuantos_epochs = 1 #@param {type:\"number\"}\n",
        "save_every_n_epochs = guardar_cada_cuantos_epochs\n",
        "if save_every_n_epochs < 1:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "#@markdown Puedes dejar el batch size as√≠ o si tienes muchas im√°genes puedes aumentarlo.\n",
        "batch_size = 2 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "train_batch_size = batch_size\n",
        "#@markdown Pasos de \"calentamiento\" durante el entrenamiento. Recomiendo dejarlo en 5%.\n",
        "calentamiento = 0.05 #@param {type:\"slider\", min:0.0, max:0.5, step:0.01}\n",
        "lr_warmup_ratio = calentamiento\n",
        "lr_warmup_steps = 0\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Entrenamiento\n",
        "#@markdown La tasa de aprendizaje (unet) es lo m√°s importante. Puedes dejarlo como est√° y probar. Si tienes muy pocas im√°genes prueba 1e-3. Si tienes muchas im√°genes y quieres entrenar m√°s despacio prueba 1e-4.<p>\n",
        "#@markdown El text encoder ayuda a tu Lora a aprender conceptos un poco mejor. Se recomienda la mitad o un quinto del unet.\n",
        "aprendizaje_unet = 5e-4 #@param {type:\"number\"}\n",
        "unet_lr = aprendizaje_unet\n",
        "aprendizaje_text_encoder = 1e-4 #@param {type:\"number\"}\n",
        "text_encoder_lr = aprendizaje_text_encoder\n",
        "#@markdown El scheduler es el algoritmo matem√°tico que guiar√° el entrenamiento. Para personajes recomiendo `cosine_with_restarts` con un valor de 3. Si no est√°s seguro ponlo en `constant` e ignora el valor.\n",
        "scheduler = \"cosine_with_restarts\" #@param [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\"]\n",
        "lr_scheduler = scheduler\n",
        "valor_de_scheduler = 3 #@param {type:\"number\"}\n",
        "lr_scheduler_number = valor_de_scheduler\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "#@markdown M√°s dim equivale a un Lora m√°s grande. Podr√° aprender m√°s como tambi√©n aprender m√°s basura. Antes se usaba el dim 128, pero ahora se recomienda 16 o 32 con una tasa de aprendizaje mayor. <p>\n",
        "#@markdown El alpha se recomienda que sea igual o la mitad del dim, o 1. <p>\n",
        "#@markdown Puedes dejar ambos como est√°n.\n",
        "network_dim = 16 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "network_alpha = 8 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Tipo de Lora\n",
        "#@markdown LoCon y LoHa son nuevos tipos de LoRA que ofrecen mayor aprendizaje. Si deseas experimentar con ellos hazlo aqu√≠, de otra forma no te preocupes.\n",
        "#@markdown Se dice que los LoCon son buenos para los estilos. M√°s informaci√≥n [aqu√≠](https://github.com/KohakuBlueleaf/Lycoris). Necesitar√°s [esta extensi√≥n](https://github.com/KohakuBlueleaf/a1111-sd-webui-locon) para usarlos en webui.\n",
        "#@markdown Nota: La `conv_compression` hace el archivo m√°s peque√±o pero puede tener resultados inesperados. <p>\n",
        "#@markdown Valores recomendados:\n",
        "\n",
        "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
        "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
        "#@markdown | LoRA | 32 | 16 | - | - |\n",
        "#@markdown | LoCon | 16 | 8 | 8 | 1 |\n",
        "#@markdown | LoHa | 8 | 4 | 4 | 1 |\n",
        "\n",
        "lora_type = \"LoRA\" #@param [\"LoRA\", \"LoCon Kohya\", \"LoCon Lycoris\", \"LoHa Lycoris\"]\n",
        "conv_dim = 8 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "conv_alpha = 1 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "conv_compression = False #@param {type:\"boolean\"}\n",
        "\n",
        "network_module = \"lycoris.kohya\" if \"Lycoris\" in lora_type else \"networks.lora\"\n",
        "network_args = None if lora_type == \"LoRA\" else [\n",
        "  f\"conv_dim={conv_dim}\",\n",
        "  f\"conv_alpha={conv_alpha}\",\n",
        "]\n",
        "if \"Lycoris\" in lora_type:\n",
        "  network_args.append(f\"algo={'loha' if 'LoHa' in network_args else 'lora'}\")\n",
        "  network_args.append(f\"disable_conv_cp={str(not conv_compression)}\")\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Listo\n",
        "#@markdown Ahora puedes correr esta celda apretando el bot√≥n circular a la izquierda. ¬°Buena suerte!\n",
        "\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir,\"deps\")\n",
        "repo_dir = os.path.join(root_dir,\"kohya-trainer\")\n",
        "\n",
        "main_dir = os.path.join(root_dir,\"drive/MyDrive/lora_training\")\n",
        "config_dir = os.path.join(main_dir,\"config\")\n",
        "datasets_dir = os.path.join(main_dir,\"datasets\")\n",
        "output_dir = os.path.join(main_dir,\"output\")\n",
        "logging_dir = os.path.join(main_dir,\"log\")\n",
        "\n",
        "images_folder = os.path.join(datasets_dir, project_name)\n",
        "output_folder = os.path.join(output_dir, project_name)\n",
        "config_folder = os.path.join(config_dir, project_name)\n",
        "\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "tools_dir = os.path.join(repo_dir,\"tools\")\n",
        "finetune_dir = os.path.join(repo_dir,\"finetune\")\n",
        "\n",
        "images = None\n",
        "dataset_config_file = None\n",
        "config_file = None\n",
        "  \n",
        "def clone_repo():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone https://github.com/Linaqruf/kohya-trainer {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  !git reset --hard 78624a16498ebcb323963e49fbf470b997c848df\n",
        "\n",
        "def install_ubuntu_deps(url, name, dst):\n",
        "    os.chdir(repo_dir)\n",
        "    !wget -q --show-progress {url}\n",
        "    with zipfile.ZipFile(name, \"r\") as deps:\n",
        "        deps.extractall(dst)\n",
        "    !dpkg -i {dst}/*\n",
        "    os.remove(name)\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "def install_dependencies():\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !pip -q install --upgrade -r requirements.txt\n",
        "  !pip install xformers triton\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "  !sed -i 's/model_name + \".\"/model_name + \"-{:02d}.\".format(num_train_epochs)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config):\n",
        "    write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "def validate_dataset():\n",
        "  global images, lr_warmup_steps, lr_warmup_ratio\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\")\n",
        "\n",
        "  print(\"\\nüíø Revisando archivos...\")\n",
        "  if not project_name.strip():\n",
        "    print(\"üí• Error: Por favor elige un nombre de proyecto.\")\n",
        "    return\n",
        "\n",
        "  # this is starting to spaghetti\n",
        "  if \"override_dataset_config_file\" in globals() and override_dataset_config_file:\n",
        "    try:\n",
        "      datconf = toml.load(override_dataset_config_file)\n",
        "      datasets = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datconf[\"datasets\"][0][\"subsets\"]}\n",
        "    except:\n",
        "      print(f\"üí• Error: Tu configuraci√≥n de dataset propia es inv√°lida!\")\n",
        "      return\n",
        "    folders = datasets.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets[folder]) for folder in folders}\n",
        "  else:\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"üí• Error: La carpeta {folder.replace('/content/drive/', '')} no existe.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"üí• Error: La carpeta {folder.replace('/content/drive/', '')} est√° vac√≠a.\")\n",
        "      return\n",
        "  if not [txt for txt in files if txt.lower().endswith(\".txt\")]:\n",
        "    print(\"üí• Error: No tienes archivos de texto junto a tus im√°genes. Por favor describe al menos 1 de tus im√°genes para continuar.\")\n",
        "    return\n",
        "  for f in files:\n",
        "    if not f.lower().endswith(\".txt\") and not f.lower().endswith(supported_types):\n",
        "      print(f\"üí• Error: Archivo inv√°lido encontrado: \\\"{f}\\\". Abortando.\")\n",
        "      return\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = int(max_train_epochs*steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps * lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"üìÅ\"+folder.replace(\"/content/drive/\", \"\"))\n",
        "    print(f\"üìà Se encontraron {img} im√°genes con {rep} repeticiones, resultando en {img*rep} pasos.\")\n",
        "  print(f\"üìâ Divide {pre_steps_per_epoch} pasos en {train_batch_size} batch size para resultar en {steps_per_epoch} pasos por epoch.\")\n",
        "  print(f\"üîÆ Habr√° {max_train_epochs} epochs, para un total de {total_steps} pasos de entrenamiento.\")\n",
        "\n",
        "  if total_steps > 10000:\n",
        "    print(\"üí• Error: Tienes muchos pasos. Probablemente te equivocaste. Abortando...\") \n",
        "    return\n",
        "  return True\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "  import toml\n",
        "\n",
        "  os.makedirs(os.path.join(config_dir, project_name), exist_ok=True)\n",
        "  dataset_config_file = os.path.join(config_dir, project_name, \"dataset_config.toml\")\n",
        "  config_file = os.path.join(config_dir, project_name, \"training_config.toml\")\n",
        "\n",
        "  if \"override_config_file\" in globals() and override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"‚≠ï Usando configuraci√≥n propia {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"additional_network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": True if text_encoder_lr == 0 else None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"optimizer_type\": \"AdamW8bit\",\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"noise_offset\": None,\n",
        "        \"clip_skip\": 2,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": True,\n",
        "        \"lowram\": True,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"mixed_precision\": \"fp16\",\n",
        "        \"output_dir\": output_folder,\n",
        "        \"logging_dir\": logging_dir,\n",
        "        \"output_name\": project_name,\n",
        "        \"log_prefix\": project_name,\n",
        "      },\n",
        "      \"model_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_model_as\": \"safetensors\"\n",
        "      },\n",
        "      \"dreambooth_arguments\": {\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "      },\n",
        "      \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "      },\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"üìÑ Configuraci√≥n guardada en {config_file}\")\n",
        "\n",
        "  if \"override_dataset_config_file\" in globals() and override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"‚≠ï Usando configuraci√≥n de archivos propia {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_caption,\n",
        "        \"flip_aug\": flip_aug,        \n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "        \"max_bucket_reso\": 1280 if resolution > 640 else 1024,   \n",
        "      },\n",
        "      \"datasets\": [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"keep_tokens\": keep_tokens,\n",
        "              \"image_dir\": images_folder\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"üìÑ Configuraci√≥n de archivos guardada en {dataset_config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file\n",
        "  real_model_url = model_url.strip()\n",
        "  \n",
        "  if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "    model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "  else:\n",
        "    model_file = \"/content/downloaded_model.safetensors\"\n",
        "    if os.path.exists(model_file):\n",
        "      !rm \"{model_file}\"\n",
        "      \n",
        "  if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
        "    real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "  elif m := re.search(r\"(?:https?://)?(?:www\\.)?civitai\\.com/models/([0-9]+)\", model_url):\n",
        "    real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "\n",
        "  !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "  if model_file.lower().endswith(\".safetensors\"):\n",
        "    from safetensors.torch import load_file as load_safetensors\n",
        "    try:\n",
        "      test = load_safetensors(model_file)\n",
        "      del test\n",
        "    except Exception as e:\n",
        "      #if \"HeaderTooLarge\" in str(e):\n",
        "      new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "      !mv \"{model_file}\" \"{new_model_file}\"\n",
        "      model_file = new_model_file\n",
        "      print(f\"El modelo ahora es {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "  if model_file.lower().endswith(\".ckpt\"):\n",
        "    from torch import load as load_ckpt\n",
        "    try:\n",
        "      test = load_ckpt(model_file)\n",
        "      del test\n",
        "    except Exception as e:\n",
        "      return False\n",
        "      \n",
        "  return True\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    print(\"üìÇ Conecctando a Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "  \n",
        "  for dir in [deps_dir, repo_dir, main_dir, config_dir, datasets_dir, output_dir, logging_dir, images_folder, output_folder, config_folder]:\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "  \n",
        "  if not dependencies_installed:\n",
        "    print(\"\\nüè≠ Instalando...\\n\")\n",
        "    clone_repo()\n",
        "    #!apt -y update -qq\n",
        "    #!apt install libunwind8=1.2.1-9build1\n",
        "    #install_ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/ram_patch.zip\", \"ram_patch.zip\", deps_dir)\n",
        "    #%env LD_PRELOAD=libtcmalloc.so\n",
        "    install_ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/deb-libs.zip\", \"deb-libs.zip\", deps_dir)\n",
        "    install_dependencies()\n",
        "    dependencies_installed = True\n",
        "    print(\"\\n‚úÖ Instalaci√≥n completada.\")\n",
        "  else:\n",
        "    print(\"‚úÖ Ya se ha realizado la instalaci√≥n.\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\nüîÑ Descargando modelo...\")\n",
        "    if not download_model():\n",
        "      print(\"\\nüí• Error: El modelo es inv√°lido o incorrecto. Recomiendo un enlace de huggingface o civitai.\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\nüîÑ El modelo ya est√° descargado.\\n\")\n",
        "\n",
        "  create_config()\n",
        "  \n",
        "  print(\"\\n‚≠ê Abriendo entrenador...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "  \n",
        "  !accelerate launch --config_file={accelerate_config} --num_cpu_threads_per_process=1 train_network.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    display(Markdown(\"### ‚úÖ ¬°Listo! [Puedes descargar tus Loras desde tu Google Drive](https://drive.google.com/drive/my-drive)\"))\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNM2AAWaNheJ"
      },
      "source": [
        "## *Ô∏è‚É£ Extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jhfwm-iPNiNh"
      },
      "outputs": [],
      "source": [
        "#@markdown ### üìÇ Extraer datos\n",
        "#@markdown Es lento subir muchos archivos peque√±os, si quieres puedes subir un zip y extraerlo aqu√≠.\n",
        "zip = \"/content/drive/MyDrive/lora_training/datasets/warrior.zip\" #@param {type:\"string\"}\n",
        "extract_to = \"/content/drive/MyDrive/lora_training/datasets/\" #@param {type:\"string\"}\n",
        "\n",
        "import os, zipfile\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  print(\"üìÇ Connecting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as f:\n",
        "  f.extractall(extract_to)\n",
        "\n",
        "print(\"‚úÖ Done\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rXRAOL7R8ofB"
      },
      "outputs": [],
      "source": [
        "#@markdown ### üî¢ Contar archivos\n",
        "#@markdown Google Drive hace imposible contar los archivos en una carpeta, por lo que aqu√≠ puedes ver la cantidad de archivos en carpetas y subcarpetas.\n",
        "carpeta = \"/content/drive/MyDrive/lora_training/datasets\" #@param {type:\"string\"}\n",
        "folder = carpeta\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"üìÇ Conectando a Google Drive...\\n\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "tree = {}\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder)):\n",
        "  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images and not captions and not others \\\n",
        "                    else f\"{images:>4} images | {captions:>4} text files | {others:>4} other files\"\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"üìÅ{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Os2Hb3CG33Vj"
      },
      "source": [
        "### üìö Load custom config file\n",
        "\n",
        "**WARNING!** Advanced users only. <p>\n",
        "\n",
        "Changing the `dataset_config_file` will allow you to divide your dataset into subsets, and more.\n",
        "\n",
        "You can also make one of them a regularisation dataset with `is_reg = true`.\n",
        "\n",
        "Edit and run the below cell to apply a custom config. Run the cell below it to undo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqnJP6dZAXk-"
      },
      "outputs": [],
      "source": [
        "dataset_config = \"\"\"\n",
        "[[datasets]]\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/lora_training/datasets/project_name/good_images\"\n",
        "num_repeats = 3\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/lora_training/datasets/project_name/bad_images\"\n",
        "num_repeats = 1\n",
        "\n",
        "[general]\n",
        "keep_tokens = 1\n",
        "resolution = 512\n",
        "shuffle_caption = true\n",
        "flip_aug = true\n",
        "caption_extension = \".txt\"\n",
        "enable_bucket = true\n",
        "bucket_reso_steps = 64\n",
        "bucket_no_upscale = false\n",
        "min_bucket_reso = 256\n",
        "max_bucket_reso = 1024\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/dataset_config.toml\", \"w\") as f:\n",
        "  f.write(dataset_config)\n",
        "\n",
        "override_dataset_config_file = \"/content/dataset_config.toml\"\n",
        "override_config_file = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zlm23ssd3_Z2"
      },
      "outputs": [],
      "source": [
        "override_dataset_config_file = None\n",
        "override_config_file = None"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
