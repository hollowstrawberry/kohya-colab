{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚≠ê Lora Trainer by Hollowstrawberry\n",
        "\n",
        "Here is a [guide to using this colab](https://civitai.com/models/22530). It will help you make a dataset quickly and using it to train a Lora.\n",
        "\n",
        "This is based on the work of [Kohya-ss and Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb). Thank you!\n"
      ],
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|Colab|English|Spanish|\n",
        "|:--|:-:|:-:|\n",
        "| üìä **Dataset Maker** | <a target=\"_blank\" href=\"https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a> | [üá™üá∏](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) |\n",
        "| ‚≠ê **Lora Trainer** | <a target=\"_blank\" href=\"https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a> | [üá™üá∏](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) |"
      ],
      "metadata": {
        "id": "dPQlB4djNm3C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OglZzI_ujZq-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ## üö© Start Here\n",
        "import os\n",
        "import re\n",
        "import toml\n",
        "import shutil\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "from google.colab import output as console\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Setup\n",
        "#@markdown Your project name will be the same as the folder containing your images. Spaces aren't allowed.\n",
        "project_name = \"\" #@param {type:\"string\"}\n",
        "#@markdown Decide the model that will be downloaded and used for training. The base models produce the cleanest and most consistent results. You can instead specify a custom model if you're really sure.\n",
        "training_model = \"Anime (animefull-final-pruned-fp16.safetensors)\" #@param [\"Anime (animefull-final-pruned-fp16.safetensors)\", \"Photorealism (sd-v1-5-pruned-noema-fp16.safetensors)\"]\n",
        "custom_model_url = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if custom_model_url:\n",
        "  model_url = custom_model_url\n",
        "elif \"Anime\" in training_model:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n",
        "else:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Files <p>\n",
        "#@markdown If you used [my dataset maker](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb#scrollTo=-rdgF2AWLS2h), you're ready to go. Otherwise, create a folder in your Google Drive like this: `lora_training/datasets/project_name` and fill it with your images and their descriptions. You may use the Extras at the bottom to extract a zip file. <p>\n",
        "#@markdown Resolution of 512 is standard for Stable Diffusion 1.5. Images will be automatically scaled while training, you don't need to crop or resize anything.\n",
        "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:256}\n",
        "#@markdown This option will train your images both normally and flipped, for no extra cost, to learn more from them. Turn it on specially if you have less than 20 images. <p> \n",
        "#@markdown **Turn it off if you care about asymmetrical elements in your Lora**.\n",
        "flip_aug = True #@param {type:\"boolean\"}\n",
        "#@markdown If you have an activation tag at the start of every text file, increase `keep_tokens` to 1.\n",
        "keep_tokens = 0 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "#@markdown If your text files use anime tags keep this active.\n",
        "shuffle_caption = True #@param {type:\"boolean\"}\n",
        "caption_extension = \".txt\"\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Steps <p>\n",
        "#@markdown Your images will repeat this number of times during training. I recommend that your images multiplied by their repeats is between 200 and 400.\n",
        "num_repeats = 10 #@param {type:\"number\"}\n",
        "#@markdown One epoch is a number of training steps equal to: your number of images multiplied by their repeats, divided by batch size. <p>\n",
        "#@markdown More epochs will give your Lora more time to learn and more options for you to test. If you followed the rest of the instructions, I recommend 10 to 30 epochs which would result in 1000 to 6000 total training steps. You'll see your total steps when the training begins.\n",
        "max_train_epochs = 15 #@param {type:\"number\"}\n",
        "save_every_n_epochs = 1 #@param {type:\"number\"}\n",
        "if save_every_n_epochs < 1:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "#@markdown Increasing the batch size may help for lots of images, but you can leave it as is.\n",
        "train_batch_size = 2 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Training\n",
        "#@markdown The learning rate is the most important for your results. If you want to train slower with lots of images, or if your dim and alpha are high, move the unet to 1e-4 or lower. <p>\n",
        "#@markdown The text encoder helps your Lora learn concepts slightly better. It is recommended to make it half or a fifth of the unet. If you're training a style you may set it to 0.\n",
        "unet_lr = 5e-4 #@param {type:\"number\"}\n",
        "text_encoder_lr = 1e-4 #@param {type:\"number\"}\n",
        "#@markdown The scheduler is the algorithm that guides the learning rate. If you're not sure, pick `constant` and ignore the number. I personally recommend `cosine_with_restarts` with 3 restarts.\n",
        "lr_scheduler = \"cosine_with_restarts\" #@param [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\"]\n",
        "lr_scheduler_number = 3 #@param {type:\"number\"}\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "#@markdown Steps spent \"warming up\" the learning rate during training for efficiency. I recommend leaving it at 5%.\n",
        "lr_warmup_ratio = 0.05 #@param {type:\"slider\", min:0.0, max:0.5, step:0.01}\n",
        "lr_warmup_steps = 0\n",
        "#@markdown More dim means larger Lora, it can hold more information but can also hold more garbage. A dim between 8-32 is recommended. The standard used to be 128, but it's completely overkill, as long as you increase the learning rate to preserve the detail. <p>\n",
        "#@markdown Alpha is recommended to be equal or half the dim, or 1. <p>\n",
        "#@markdown You can leave both as is.\n",
        "network_dim = 16 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "network_alpha = 8 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Lora Type\n",
        "#@markdown LoCon and LoHa are new types of LoRA with different/expanded capacity for learning. If you want to experiment with them go ahead, otherwise **don't worry about it!** <p>\n",
        "#@markdown LoCons are said to be great with artstyles. If you select LoRA then the following 2 values do nothing. More info [here](https://github.com/KohakuBlueleaf/Lycoris). You'll need [this extension](https://github.com/KohakuBlueleaf/a1111-sd-webui-locon) to use them in webui. <p>\n",
        "#@markdown Recommended values (from lycoris repo):\n",
        "\n",
        "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
        "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
        "#@markdown | LoRA | 32 | 16 | - | - |\n",
        "#@markdown | LoCon | 16 | 8 | 8 | 1 |\n",
        "#@markdown | LoHa | 8 | 4 | 4 | 1 |\n",
        "\n",
        "lora_type = \"LoRA\" #@param [\"LoRA\", \"LoCon Kohya\", \"LoCon Lycoris\", \"LoHa Lycoris\"]\n",
        "conv_dim = 8 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "conv_alpha = 1 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "conv_compression = False #@param {type:\"boolean\"}\n",
        "\n",
        "network_module = \"lycoris.kohya\" if \"Lycoris\" in lora_type else \"networks.lora\"\n",
        "network_args = None if lora_type == \"LoRA\" else [\n",
        "  f\"conv_dim={conv_dim}\",\n",
        "  f\"conv_alpha={conv_alpha}\",\n",
        "]\n",
        "if \"Lycoris\" in lora_type:\n",
        "  network_args.append(f\"algo={'loha' if 'LoHa' in network_args else 'lora'}\")\n",
        "  network_args.append(f\"disable_conv_cp={str(not conv_compression)}\")\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Ready\n",
        "#@markdown You can now run this cell to cook your Lora. Good luck! <p>\n",
        "#markdown Save additional data equaling ~500 MB allowing you to resume training later.\n",
        "save_state = False #param {type:\"boolean\"}\n",
        "#markdown Resume training if a save state is found.\n",
        "resume = False #param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "# üë©‚Äçüíª Cool code goes here\n",
        "\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir,\"deps\")\n",
        "repo_dir = os.path.join(root_dir,\"kohya-trainer\")\n",
        "\n",
        "main_dir = os.path.join(root_dir,\"drive/MyDrive/lora_training\")\n",
        "config_dir = os.path.join(main_dir,\"config\")\n",
        "datasets_dir = os.path.join(main_dir,\"datasets\")\n",
        "output_dir = os.path.join(main_dir,\"output\")\n",
        "logging_dir = os.path.join(main_dir,\"log\")\n",
        "\n",
        "images_folder = os.path.join(datasets_dir, project_name)\n",
        "output_folder = os.path.join(output_dir, project_name)\n",
        "config_folder = os.path.join(config_dir, project_name)\n",
        "\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "tools_dir = os.path.join(repo_dir,\"tools\")\n",
        "finetune_dir = os.path.join(repo_dir,\"finetune\")\n",
        "\n",
        "images = None\n",
        "dataset_config_file = None\n",
        "config_file = None\n",
        "  \n",
        "def clone_repo():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone https://github.com/Linaqruf/kohya-trainer {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  !git reset --hard 86de685a8c37e60a610d08cbece3da6b3a553bc0\n",
        "\n",
        "def install_ubuntu_deps(url, name, dst):\n",
        "    os.chdir(repo_dir)\n",
        "    !wget -q --show-progress {url}\n",
        "    with zipfile.ZipFile(name, \"r\") as deps:\n",
        "        deps.extractall(dst)\n",
        "    !dpkg -i {dst}/*\n",
        "    os.remove(name)\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "def install_dependencies():\n",
        "  os.chdir(repo_dir)\n",
        "  !pip -q install --upgrade -r requirements.txt\n",
        "  !pip install -q xformers==\"0.0.17.dev476\"\n",
        "  !pip install -q triton==\"2.0.0.post1\"\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "  !sed -i 's/model_name + \".\"/model_name + \"-{:02d}.\".format(num_train_epochs)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config):\n",
        "    write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "def validate_dataset():\n",
        "  global images, lr_warmup_steps, lr_warmup_ratio\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\")\n",
        "\n",
        "  print(\"\\nüíø Checking dataset...\")\n",
        "  if not project_name.strip():\n",
        "    print(\"üí• Error: Please choose a project name.\")\n",
        "    return\n",
        "\n",
        "  # this is starting to spaghetti\n",
        "  if \"override_dataset_config_file\" in globals() and override_dataset_config_file:\n",
        "    try:\n",
        "      datconf = toml.load(override_dataset_config_file)\n",
        "      datasets = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datconf[\"datasets\"][0][\"subsets\"]}\n",
        "    except:\n",
        "      print(f\"üí• Error: Your custom dataset config file is invalid!\")\n",
        "      return\n",
        "    folders = datasets.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.endswith(supported_types)]), datasets[folder]) for folder in folders}\n",
        "  else:\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"üí• Error: The folder {folder.replace('/content/drive/', '')} doesn't exist.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"üí• Error: Your {folder.replace('/content/drive/', '')} folder is empty.\")\n",
        "      return\n",
        "  if not [txt for txt in files if txt.endswith(\".txt\")]:\n",
        "    print(\"üí• Error: You don't have text files with captions next to your images. If you want to proceed, add a caption to at least 1 of your images.\")\n",
        "    return\n",
        "  for f in files:\n",
        "    if not f.endswith(\".txt\") and not f.endswith(supported_types):\n",
        "      print(f\"üí• Error: Invalid file in dataset: \\\"{f}\\\". Aborting.\")\n",
        "      return\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = int(max_train_epochs*steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps * lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"üìÅ\"+folder.replace(\"/content/drive/\", \"\"))\n",
        "    print(f\"üìà Found {img} images with {rep} repeats, equaling {img*rep} steps\")\n",
        "  print(f\"üìâ Divide {pre_steps_per_epoch} steps by {train_batch_size} batch size to get {steps_per_epoch} steps per epoch.\")\n",
        "  print(f\"üîÆ There will be {max_train_epochs} epochs, for around {total_steps} total training steps.\")\n",
        "\n",
        "  if total_steps > 10000:\n",
        "    print(\"üí• Error: Your total steps are too high. You probably made a mistake. Aborting...\") \n",
        "    return\n",
        "  return True\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  os.makedirs(os.path.join(config_dir, project_name), exist_ok=True)\n",
        "  dataset_config_file = os.path.join(config_dir, project_name, \"dataset_config.toml\")\n",
        "  config_file = os.path.join(config_dir, project_name, \"training_config.toml\")\n",
        "\n",
        "  if resume:\n",
        "    resume_points = [f.path for f in os.scandir(output_folder) if f.is_dir()]\n",
        "    resume_points.sort()\n",
        "    last_resume_point = resume_points[-1] if resume_points else None\n",
        "  else:\n",
        "    last_resume_point = None\n",
        "\n",
        "  if \"override_config_file\" in globals() and override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"‚≠ï Using custom config file {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"additional_network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": True if text_encoder_lr == 0 else None,\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"optimizer_type\": \"AdamW8bit\",\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"noise_offset\": None,\n",
        "        \"clip_skip\": 2,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": True,\n",
        "        \"lowram\": True,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"mixed_precision\": \"fp16\",\n",
        "        \"output_dir\": output_folder,\n",
        "        \"logging_dir\": logging_dir,\n",
        "        \"output_name\": project_name,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"save_state\": save_state,\n",
        "        \"save_last_n_epochs_state\": 1 if save_state else None,\n",
        "        \"resume\": last_resume_point\n",
        "      },\n",
        "      \"model_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "      },\n",
        "      \"dreambooth_arguments\": {\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "      },\n",
        "      \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "      },\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"üìÑ Config saved to {config_file}\")\n",
        "\n",
        "  if \"override_dataset_config_file\" in globals() and override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"‚≠ï Using custom dataset config file {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_caption,\n",
        "        \"flip_aug\": flip_aug,        \n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "        \"max_bucket_reso\": 1280 if resolution > 640 else 1024,   \n",
        "      },\n",
        "      \"datasets\": [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"keep_tokens\": keep_tokens,\n",
        "              \"image_dir\": images_folder\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"üìÑ Dataset config saved to {dataset_config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file\n",
        "  real_model_url = model_url.strip()\n",
        "\n",
        "  if not real_model_url:\n",
        "    real_model_url = \"https://huggingface.co/hollowstrawberry/animemodel/resolve/main/model.safetensors\"\n",
        "  \n",
        "  if real_model_url.endswith((\".ckpt\", \".safetensors\")):\n",
        "    model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "  else:\n",
        "    model_file = \"/content/downloaded_model.safetensors\"\n",
        "\n",
        "  if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
        "    real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "  elif m := re.search(r\"(?:https?://)?(?:www\\.)?civitai\\.com/models/([0-9]+)\", model_url):\n",
        "    real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "\n",
        "  !wget \"{real_model_url}\" -O \"{model_file}\"\n",
        "\n",
        "  if model_file.endswith(\".safetensors\"):\n",
        "    from safetensors.torch import load_file as load_safetensors\n",
        "    try:\n",
        "      test = load_safetensors(model_file)\n",
        "      del test\n",
        "    except Exception as e:\n",
        "      #if \"HeaderTooLarge\" in str(e):\n",
        "      new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "      !mv \"{model_file}\" \"{new_model_file}\"\n",
        "      model_file = new_model_file\n",
        "      print(f\"Renamed model to {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "  if model_file.endswith(\".ckpt\"):\n",
        "    from torch import load as load_ckpt\n",
        "    try:\n",
        "      test = load_ckpt(model_file)\n",
        "      del test\n",
        "    except Exception as e:\n",
        "      return False\n",
        "      \n",
        "  return True\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    print(\"üìÇ Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "  \n",
        "  for dir in [deps_dir, repo_dir, main_dir, config_dir, datasets_dir, output_dir, logging_dir, images_folder, output_folder, config_folder]:\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "  \n",
        "  if not dependencies_installed:\n",
        "    print(\"\\nüè≠ Installing dependencies...\\n\")\n",
        "    clone_repo()\n",
        "    !apt -y update -qq\n",
        "    install_ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/ram_patch.zip\", \"ram_patch.zip\", deps_dir)\n",
        "    %env LD_PRELOAD=libtcmalloc.so\n",
        "    install_ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/deb-libs.zip\", \"deb-libs.zip\", deps_dir)\n",
        "    install_dependencies()\n",
        "    console.clear()\n",
        "    print(\"‚úÖ Installation finished.\")\n",
        "    dependencies_installed = True\n",
        "  else:\n",
        "    print(\"\\n‚úÖ Dependencies already installed.\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\nüîÑ Downloading model... Don't interrupt this at any cost...\\n\")\n",
        "    if not download_model():\n",
        "      print(\"\\nüí• Error: The model you selected is invalid or corrupted. You can use a civitai or huggingface link, or any direct download link.\")\n",
        "      return\n",
        "  else:\n",
        "    print(\"\\nüîÑ Model already downloaded.\\n\")\n",
        "\n",
        "  create_config()\n",
        "  \n",
        "  print(\"\\n‚≠ê Starting trainer...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "  \n",
        "  !accelerate launch --config_file={accelerate_config} --num_cpu_threads_per_process=1 train_network.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    display(Markdown(\"### ‚úÖ Done! [Go download your Lora(s) from Google Drive](https://drive.google.com/drive/my-drive)\"))\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Ô∏è‚É£ Extras"
      ],
      "metadata": {
        "id": "mBMUJ7BuvNcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### üìÇ Unzip dataset\n",
        "#@markdown It's much slower to upload individual files to your Drive, so you may want to upload a zip if you have your dataset in your computer.\n",
        "zip = \"/content/drive/MyDrive/lora_training/datasets/example.zip\" #@param {type:\"string\"}\n",
        "extract_to = \"/content/drive/MyDrive/lora_training/datasets/example/\" #@param {type:\"string\"}\n",
        "\n",
        "import os, zipfile\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  print(\"üìÇ Connecting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as f:\n",
        "  f.extractall(extract_to)\n",
        "\n",
        "print(\"‚úÖ Done\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WDjkp4scvPgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### üî¢ Count datasets\n",
        "#@markdown Google Drive makes it impossible to count the files in a folder, so this will show you the file counts in all folders and subfolders.\n",
        "folder = \"/content/drive/MyDrive/lora_training/datasets\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"üìÇ Connecting to Google Drive...\\n\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "tree = {}\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder)):\n",
        "  images = len([f for f in files if f.endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images and not captions and not others \\\n",
        "                    else f\"{images:>4} images | {captions:>4} text files | {others:>4} other files\"\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"üìÅ{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))\n"
      ],
      "metadata": {
        "id": "aKWlpsG0jrX3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### üìö Load custom config file\n",
        "#@markdown **WARNING!** Advanced users only. <p>\n",
        "#@markdown Changing the `dataset_config_file` will allow you to use multiple datasets at once, with different repeats, keep_tokens, etc. <p>\n",
        "#@markdown Changing the `config_file` will override ALL settings including project name and model name, but will allow you to use any obscure settings you want about the kohya trainer. <p>\n",
        "#@markdown The configs are in `.toml` format, which this trainer saves into the `lora_training/config` folder before training.\n",
        "#@markdown Example dataset config: <p>\n",
        "#@markdown ```toml\n",
        "#@markdown [[datasets]]\n",
        "#@markdown \n",
        "#@markdown [[datasets.subsets]]\n",
        "#@markdown image_dir = \"/content/drive/MyDrive/lora_training/datasets/mylora_good\"\n",
        "#@markdown num_repeats = 5\n",
        "#@markdown keep_tokens = 1\n",
        "#@markdown \n",
        "#@markdown [[datasets.subsets]]\n",
        "#@markdown image_dir = \"/content/drive/MyDrive/lora_training/datasets/mylora_bad\"\n",
        "#@markdown num_repeats = 1\n",
        "#@markdown keep_tokens = 0\n",
        "#@markdown #is_reg = true # Add this to use as regularization images\n",
        "#@markdown \n",
        "#@markdown [general]\n",
        "#@markdown resolution = 512\n",
        "#@markdown shuffle_caption = true\n",
        "#@markdown flip_aug = true\n",
        "#@markdown caption_extension = \".txt\"\n",
        "#@markdown enable_bucket = true\n",
        "#@markdown bucket_reso_steps = 64\n",
        "#@markdown bucket_no_upscale = false\n",
        "#@markdown min_bucket_reso = 256\n",
        "#@markdown max_bucket_reso = 1024\n",
        "#@markdown ```\n",
        "#@markdown Run this with empty values to disable custom configs.\n",
        "override_dataset_config_file = \"/content/drive/MyDrive/lora_training/datasets/mylora_dataset_config.toml\" #@param {type:\"string\"}\n",
        "override_config_file = \"\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Y037lagnJWmn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
